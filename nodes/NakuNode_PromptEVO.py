# -*- coding: utf-8 -*-
import random
import json
import time
import io
import base64
import urllib3
import ssl

# Disable SSL warnings for self-signed certificates
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# å°è¯•å¯¼å…¥å¿…è¦çš„åº“ï¼Œå¦‚æœå¤±è´¥åˆ™åœ¨è°ƒç”¨æ—¶æŠ›å‡ºæ›´å‹å¥½çš„é”™è¯¯
try:
    from PIL import Image
except ImportError:
    Image = None

try:
    import requests
except ImportError:
    requests = None

# Import API utils
import sys
import os
sys.path.insert(0, os.path.dirname(__file__))
from api_utils import get_api_credentials, parse_api_string_for_node

try:
    import jwt
except ImportError:
    jwt = None

try:
    from openai import OpenAI
except ImportError:
    OpenAI = None


# Built-in prompt templates
BUILTIN_PROMPTS = {
    "Qwen/Zimage": {
        "system_prompt": "ä½ æ˜¯ä¸€ä½æ‹¥æœ‰é¡¶çº§å®¡ç¾å’Œæ‘„å½±çŸ¥è¯†çš„ AI è‰ºæœ¯æ€»ç›‘ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ¥æ”¶ç”¨æˆ·çš„ç®€å•æŒ‡ä»¤ï¼Œå¯¹æŒ‡ä»¤è¿›è¡Œå»¶å±•ï¼Œå¢åŠ æ›´å¤šçš„ç»†èŠ‚æè¿°ã€‚æœ€ç»ˆä»¥ [ç”»é¢é£æ ¼] +[ä¸»ä½“æè¿°] +[åœºæ™¯æè¿°]+ [ç»†èŠ‚ä¿®é¥°] + [ç”»é¢æ„å›¾] + [å…‰çº¿ä¿¡æ¯] +[ç”»é¢è‰²å½©å€¾å‘]+ [ç”»è´¨å‚æ•°] è¿™æ ·çš„æ ¼å¼è¾“å‡ºä¸€æ®µå®¢è§‚æè¿°çš„è‡ªç„¶è¯­å¥ã€‚\nç”»é¢é£æ ¼åŒ…æ‹¬ï¼šä¾‹å¦‚\"çœŸå®çš„ç”»é¢\"\"æ—¥å¼åŠ¨æ¼«\"\"å†™å®ç…§ç‰‡\"\"çœŸå®çš„å†™çœŸç…§\"ç­‰ç­‰\nä¸»ä½“æè¿°åŒ…æ‹¬ï¼šä¸»ä½“äººç‰©çš„è¯¦ç»†æè¿°ï¼ˆåœ°åŸŸã€æ€§åˆ«ã€å¹´é¾„ã€ä½“å‹ã€åŠ¨ä½œã€è¡¨æƒ…ã€æƒ…ç»ªã€æœè£…é€ å‹ï¼‰,ç”»é¢ç„¦ç‚¹çš„ä¸»ä½“æè¿°ï¼ˆç‰©ä½“/ç”Ÿç‰©çš„ç»†èŠ‚æè¿°ï¼Œä¾‹å¦‚é¢œè‰²ã€å½¢çŠ¶ã€ç»†èŠ‚ã€åŠ¨ä½œã€å¤§å°ç­‰ç­‰ï¼‰\nç»†èŠ‚ä¿®é¥°åŒ…æ‹¬ï¼šçªå‡º 1-3 ä¸ªå…³é”®ç»†èŠ‚ï¼ˆå¦‚é¢æ–™çº¹ç†ã€é¥°å“åå…‰ã€å‘ä¸é£˜åŠ¨ã€ç¯å¢ƒè™šåŒ–ç­‰ï¼‰\nç”»é¢æ„å›¾åŒ…æ‹¬ï¼šè¿ç”¨ä¸€ç§æˆ–å¤šç§ç»å…¸æ„å›¾æ³•ï¼ˆå¦‚ä¸‰åˆ†æ³•ã€æ¡†æ¶å¼ã€å¼•å¯¼çº¿ã€ä½è§’åº¦ä»°æ‹ç­‰ï¼‰,ä¼˜å…ˆä¼˜åŒ–ç”¨æˆ·æŒ‡ä»¤ä¸­çš„æ„å›¾æ•ˆæœï¼Œè‹¥ç”¨æˆ·æœªæŒ‡å®šï¼Œåˆ™è¾“å‡ºæœ€å…·ç¾æ„Ÿçš„æ„å›¾ã€‚\nå…‰çº¿ä¿¡æ¯åŒ…æ‹¬ï¼šæ ¹æ®æ°›å›´å’Œåœºæ™¯ï¼Œé€‰æ‹©æœ€åŒ¹é…çš„å…‰çº¿ç±»å‹ï¼ˆå¦‚ä¾§é€†å…‰ã€é»„é‡‘æ—¶åˆ»é€†å…‰ã€æ¼«å°„å…‰ç­‰ï¼‰ã€‚ä¼˜å…ˆä¼˜åŒ–ç”¨æˆ·æŒ‡ä»¤ä¸­çš„å…‰çº¿æ•ˆæœï¼Œä¾‹å¦‚ç”¨æˆ·è¾“å…¥\"é€†å…‰\"ã€‚ä¼˜åŒ–ç»“æœåº”ä¸ºï¼š\"ä¸€æŸä»äººç‰©ä¾§åæ–¹å‡ºç°çš„æ¸©æš–å…‰æºï¼Œå‹¾å‹’å‡ºäººç‰©çš„è„¸éƒ¨çš„è½®å»“ã€‚\"\næœ€ç»ˆè¾“å‡ºä¸€æ®µä¸å¤šäº 500 å­—çš„ä¸­æ–‡è‡ªç„¶è¯­å¥ï¼Œè¯­å¥ä¸èƒ½å‡ºç°æ¢è¡Œçš„æƒ…å†µï¼Œä¸èƒ½å‡ºç°é™¤äº†\"ï¼Œ\"\"ã€‚\"\"ï¼\"\"ï¼š\"ä»¥å¤–çš„å…¶ä»–ç¬¦å·ã€‚é¿å…ä¸­è‹±æ··æ‚ã€‚\nå¯ä»¥å¢åŠ ä¸€äº›æ°›å›´çš„æè¿°ä¾‹å¦‚\"é«˜çº§æ„Ÿ\"ã€\"ç”µå½±è°ƒè‰²\"ã€\"å†·æš–è‰²è°ƒå¯¹æ¯”\"èƒ½æœ‰æ•ˆæå‡ç”»é¢çš„è´¨æ„Ÿï¼Œé¿å…äº§ç”Ÿå»‰ä»·çš„\"ç½‘æ„Ÿ\"å›¾ç‰‡ã€‚\n# ç¤ºä¾‹å‚è€ƒï¼š\nç”¨æˆ·ï¼š\"ä¸€ä¸ªå¦–å¨†çš„å¤è£…å¥³å­\"\næœ€ç»ˆè¾“å‡ºï¼šä¸€ä½ 20 å²å·¦å³çš„ç››å”è´µå¥³ï¼ŒæŸ³å¶çœ‰ä¸¹å‡¤çœ¼ï¼Œçº¢å”‡å¾®å¯ï¼Œä¹Œé»‘é«˜é«»æ’ç‚¹ç¿ éé‡‘æ­¥æ‘‡ä¸çç æµè‹ç°ªï¼Œä½©æˆ´å¤šå±‚ç’çé¡¹åœˆï¼Œèº«ç©¿å¤§çº¢è‰²ç»‡é‡‘è¹™é‡‘ç»£é½èƒ¸è¥¦è£™ï¼Œè¡£è¥Ÿè¢–å£æ»¡å¸ƒå‡¤å‡°ç‰¡ä¸¹ç¼ æçº¹ï¼Œé‡‘çº¿ç† ç† ç”Ÿè¾‰ï¼Œè…°ç³»ç¢§ç‰è¹€èºå¸¦ï¼Œä¾§èº«å›çœ¸ä¸€æ‰‹æŠšé«»ä¸€æ‰‹æ­é›•èŠ±å±é£ï¼ŒS å‹èº«å§¿å¦–å¨†çœ¼ç¥å¦©åªšï¼ŒèƒŒæ™¯ä¸ºå”ä»£å®«å»·å†…æ®¿çƒ›å…‰æ‘‡æ›³çº±å¸˜åŠé€ï¼Œè¶…ç²¾ç»†å·¥ç¬”é‡å½©é£æ ¼å‚è€ƒã€Šç°ªèŠ±ä»•å¥³å›¾ã€‹ä¸æ•¦ç…Œå£ç”»è‰²å½©ï¼Œ8Kï¼Œé«˜æ¸…ç»†èŠ‚ã€‚\n\néœ€è¦é¿å…ä»¥ä¸‹é—®é¢˜ï¼š\n1.æ¨¡ç³Šä¸æ¸…çš„æè¿°ï¼ˆå¦‚\"å¥½çœ‹çš„ä¸œè¥¿\"ï¼‰ã€‚\n2.è‡ªç›¸çŸ›ç›¾çš„å…ƒç´ ï¼ˆå¦‚\"ç™½å¤©çš„æ»¡å¤©ç¹æ˜Ÿ\"ï¼‰ã€‚\n3.è¿‡äºå†—é•¿æˆ–å †ç Œæ— å…³è¯æ±‡ã€‚\n4.æè¿°å‡ºç°æ¢è¡Œçš„æƒ…å†µã€‚",
        "user_prompt": "{request}"
    },
    "Flux.2": {
        "system_prompt": "You are a top art director proficient in lighting, composition, color psychology, and digital rendering techniques. You deeply understand the underlying logic of top AI painting models like Midjourney and Flux. Your goal is to transform simple user concepts into **visually striking, exquisitely detailed, cinematic-quality** painting prompts.\n\n## Workflow\n1. **Analyze:** Extract the core subject, emotional tone, and scene from user input.\n2. **Enhance:** Automatically supplement missing aesthetic elements (lighting, materials, camera parameters, art style).\n3. **Structure:** Reorganize according to the golden formula: [Subject & Action] + [Environment & Context] + [Lighting & Atmosphere] + [Camera & Composition] + [Style & Medium] + [Color Palette] + [Quality Boosters].\n4. **Output:** Provide English natural language prompts. No line breaks allowed.\n\n## The Golden Formula\n[Subject & Action] + [Environment & Context] + [Lighting & Atmosphere] + [Camera & Composition] + [Style & Medium] + [Color Palette] + [Quality Boosters]\n\n## Knowledge Base\n\n### 1. Lighting (Determines Quality)\n* **Keywords:** Cinematic lighting, Volumetric lighting, Rembrandt lighting, Bioluminescence, Subsurface scattering, God rays.\n\n### 2. Composition (Determines Tension)\n* **Keywords:** Golden ratio, Rule of thirds, Low angle shot, Extreme close-up, Wide angle, Depth of field.\n\n### 3. Texture & Material (Determines Realism)\n* **Keywords:** Hyper-realistic, Intricate details, 8k texture, Unreal Engine 5 render, Ray tracing.\n\n### 4. Style Modifiers\n* **Keywords:** Cyberpunk, Steampunk, Baroque, Minimalism, Ukiyo-e, Concept art.\n\n## Rules\n1. **Language:** Always output **English** prompts in natural language. No line breaks.\n2. **No Conflicts:** Ensure style words don't conflict (e.g., don't write 'black and white' and 'rainbow' together).\n3. **Quality:** Must include quality-boosting 'spells' (Masterpiece, Best quality, Sharp focus).\n\n## Interaction Example\n**User Input:**\nCharacter: A beautiful woman in Hanfu in snow, lonely and aesthetic\n\n**Example Output:**\nA beautiful young woman in traditional Hanfu, delicate pale skin with realistic texture, sorrowful eyes looking at the distance, petite figure. Wearing a red silk Hanfu cloak, heavy fabric draped elegantly over shoulders, snowflakes melting on the fabric, distinct contrast between red cloth and white snow. Standing alone in a vast snowy landscape, minimalist composition, massive negative space, soft overcast light, muted colors with vibrant red accent, cinematic shot, depth of field, shot on 35mm film, ethereal atmosphere, ultra-detailed, 8k",
        "user_prompt": "{request}"
    }
}


class NakuNodePromptEVO:
    """
    NakuNode Prompt Evolution - Text-driven prompt generator
    """

    @classmethod
    def INPUT_TYPES(s):
        # Create AI model selection list
        model_list = ["Qwen/Zimage", "Flux.2"]

        inputs = {
            "required": {
                "ai_model": (model_list, {"default": "Qwen/Zimage"}),
                "text_request": ("STRING", {"multiline": True, "default": "Please enter your image generation request"}),
            },
            "optional": {
                "api_provider": (["SiliconFlow", "Custom"], {"default": "SiliconFlow"}),
                "siliconflow_model": (["MiniMax2.5", "GLM5", "DeepSeek3.2", "Kimi K2"], {"default": "MiniMax2.5"}),
                "custom_model": (["GPT5.2", "Gemini Pro 3.1", "Gemini Pro 3", "Claude Opus 4.6", "Qwen 3.5", "Kimi 2.5"], {"default": "GPT5.2"}),
                "api_string": ("STRING", {
                    "multiline": False,
                    "default": "",
                    "placeholder": "è¿æ¥ API Setting èŠ‚ç‚¹"
                }),
                "seed": ("INT", {"default": -1, "min": -1, "max": 0xffffffffffffffff}),
                "extra_prompts": ("STRING", {"multiline": False, "default": ""}),
            }
        }

        return inputs

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("generated_prompt",)
    FUNCTION = "generate_prompt"
    CATEGORY = "NakuNode/Prompt Generation"

    def generate_prompt(self, text_request, ai_model, api_string, seed,
                        extra_prompts="", siliconflow_model="Qwen3", custom_model="gpt_5.2", api_provider="SiliconFlow", **kwargs):
        # Set random seed
        if seed == -1:
            seed = random.randint(0, 0xffffffffffffffff)
        random.seed(seed)

        print(f"\n{'='*60}")
        print(f"[NakuNode-Prompt] Starting request - AI Model: {ai_model}")
        print(f"{'='*60}")
        print(f"[NakuNode-Prompt] Text Request: {text_request}")
        print(f"[NakuNode-Prompt] Extra Prompts: {extra_prompts}")
        
        # Parse and display extra prompts in debug format
        if extra_prompts and extra_prompts.strip():
            print(f"\n{'='*60}")
            print("[NakuNode-Prompt] === Prompt Builder Parameters ===")
            print(f"{'='*60}")
            
            # Split by dots to separate categories
            categories = extra_prompts.split('.')
            category_names = ["ğŸ“· Photography", "ğŸ‘¤ Character", "ğŸŒ Scene"]
            
            for i, cat in enumerate(categories):
                if i < len(category_names) and cat.strip():
                    print(f"\n[{category_names[i]}]")
                    # Split by comma to get individual items
                    items = [item.strip() for item in cat.split(',') if item.strip()]
                    for j, item in enumerate(items, 1):
                        print(f"  {j}. {item}")
            
            print(f"\n{'='*60}")
            
            enhanced_text_request = f"{text_request}, {extra_prompts}"
            print(f"[NakuNode-Prompt] Enhanced request: {enhanced_text_request}")
        else:
            enhanced_text_request = text_request
            print(f"[NakuNode-Prompt] No extra prompts, using original request: {enhanced_text_request}")

        # Get prompt template for selected model
        model_config = BUILTIN_PROMPTS.get(ai_model, BUILTIN_PROMPTS["Qwen/Zimage"])
        print(f"[NakuNode-Prompt] Using model config: {ai_model}")

        # Build complete prompt
        system_prompt = model_config["system_prompt"]
        user_prompt = model_config["user_prompt"].format(request=enhanced_text_request)
        print(f"[NakuNode-Prompt] System Prompt: {system_prompt[:100]}...")
        print(f"[NakuNode-Prompt] System Prompt length: {len(system_prompt)} characters")
        print(f"[NakuNode-Prompt] User Prompt length: {len(user_prompt)} characters")
        print(f"[NakuNode-Prompt] User Prompt: {user_prompt[:100]}...")

        # Select API key based on provider
        parse_api_string_for_node(api_string, "NakuNode PromptEVO")
        api_provider, api_key, api_url, sf_key, c_key, c_url = get_api_credentials(api_string, preferred_provider=api_provider)
        
        # Print API provider info
        print(f"[NakuNode-Prompt] API Provider: {api_provider}")
        print(f"[NakuNode-Prompt] SiliconFlow API Key: {'å·²è®¾ç½®' if sf_key else 'æœªè®¾ç½®'}")
        print(f"[NakuNode-Prompt] Custom API Key: {'å·²è®¾ç½®' if c_key else 'æœªè®¾ç½®'}")
        print(f"[NakuNode-Prompt] Custom API URL: {c_url}")

        # Decide whether to call API based on API provider and model selection
        if not api_key or api_key in ["Please enter SiliconFlow API Key", "Please enter your API Key", ""]:
            # Do not call API, directly return user input text with extra prompts
            print("[NakuNode-Prompt] API key is empty or not filled, returning user request directly")
            return (enhanced_text_request,)
        elif api_provider in ["SiliconFlow", "Custom"]:
            if not api_key or api_key == "Please enter SiliconFlow API Key" or api_key == "Please enter your API Key":
                print("[NakuNode-Prompt] Error: API key is empty or not filled")
                raise ValueError("When using API, please fill in your API key in the API Key field.")

            if OpenAI is None:
                print("[NakuNode-Prompt] Error: OpenAI library not installed")
                raise ImportError("Please install openai library: pip install openai")

            # SiliconFlow model mapping
            model_mapping = {
                "MiniMax2.5": "Pro/MiniMaxAI/MiniMax-M2.5",
                "GLM5": "Pro/zai-org/GLM-5",
                "DeepSeek3.2": "Pro/deepseek-ai/DeepSeek-V3.2",
                "Kimi K2": "Pro/moonshotai/Kimi-K2-Thinking"
            }

            # Custom API model mapping
            custom_model_mapping = {
                "GPT5.2": "gpt-5.2",
                "Gemini Pro 3.1": "gemini-3.1-pro-preview",
                "Gemini Pro 3": "gemini-3-pro-preview",
                "Claude Opus 4.6": "claude-opus-4-6",
                "Qwen 3.5": "qwen3.5-plus",
                "Kimi 2.5": "kimi-k2.5"
            }

            # Select model and print info based on API provider
            if api_provider == "Custom":
                selected_model = custom_model_mapping.get(custom_model, "gpt-5.2")
                # ä½¿ç”¨è§£æåçš„ c_url è€Œä¸æ˜¯åŸå§‹çš„ custom_url å‚æ•°
                api_url = c_url.rstrip('/')
                use_stream = False
                print(f"[NakuNode-Prompt] Using Custom API")
                print(f"[NakuNode-Prompt] Custom API URL: {c_url}")
                print(f"[NakuNode-Prompt] Using Custom API model: {selected_model}")
            else:
                selected_model = model_mapping.get(siliconflow_model, "Pro/MiniMaxAI/MiniMax-M2.5")
                api_url = "https://api.siliconflow.cn/v1/chat/completions"
                use_stream = True
                print(f"[NakuNode-Prompt] Using SiliconFlow API")
                print(f"[NakuNode-Prompt] SiliconFlow URL: https://api.siliconflow.cn/v1/chat/completions")
                print(f"[NakuNode-Prompt] Using SiliconFlow model: {selected_model}")

            try:
                print("[NakuNode-Prompt] Sending HTTP request...")

                # Build complete API URL for Custom
                if api_provider == "Custom" and not api_url.endswith('/v1/chat/completions'):
                    api_url = api_url + '/v1/chat/completions'

                print(f"[NakuNode-Prompt] Request URL: {api_url}")
                print(f"[NakuNode-Prompt] Stream mode: {use_stream}")

                # Use requests library to send HTTP request
                import json
                import requests

                headers = {
                    'Accept': 'application/json',
                    'Authorization': f'Bearer {api_key}',
                    'Content-Type': 'application/json'
                }

                # Print prompt content to be sent
                print(f"[NakuNode-Prompt] Sending System Prompt: {system_prompt[:200]}...")
                print(f"[NakuNode-Prompt] Sending User Prompt: {user_prompt[:200]}...")

                payload = {
                    "model": selected_model,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "n": 1,
                    "stream": use_stream,
                    "max_tokens": 2048
                }

                print("[NakuNode-Prompt] Sending request...")
                
                if use_stream:
                    # SiliconFlow stream mode
                    response = requests.post(api_url, headers=headers, json=payload, stream=True, timeout=60, verify=False)
                    print(f"[NakuNode-Prompt] HTTP status code: {response.status_code}")

                    if response.status_code == 200:
                        print("[NakuNode-Prompt] API call successful")
                        full_content = ""
                        for chunk in response.iter_lines():
                            if chunk:
                                chunk_str = chunk.decode('utf-8').replace('data: ', '')
                                if chunk_str != "[DONE]" and chunk_str.strip():
                                    try:
                                        chunk_data = json.loads(chunk_str)
                                        delta = chunk_data['choices'][0].get('delta', {})
                                        content = delta.get('content', '')
                                        if content:
                                            full_content += content
                                    except json.JSONDecodeError:
                                        continue
                        print(f"[NakuNode-Prompt] Stream response received, length: {len(full_content)}")
                        return (full_content,)
                    else:
                        error_msg = f"HTTP error: {response.status_code} - {response.text[:200]}"
                        print(error_msg)
                        return (error_msg,)
                else:
                    # Custom API non-stream mode - æ·»åŠ é‡è¯•æœºåˆ¶
                    max_retries = 3
                    retry_delay = 2  # ç§’
                    response = None
                    
                    for attempt in range(max_retries):
                        try:
                            print(f"[NakuNode-Prompt] Sending request (attempt {attempt + 1}/{max_retries})...")
                            response = requests.post(api_url, headers=headers, json=payload, timeout=60, verify=False)
                            print(f"[NakuNode-Prompt] HTTP status code: {response.status_code}")

                            if response.status_code == 200:
                                print("[NakuNode-Prompt] API call successful")
                                response_data = response.json()

                                # Parse response
                                if 'choices' in response_data and len(response_data['choices']) > 0:
                                    result = response_data['choices'][0]['message']['content']
                                    print(f"[NakuNode-Prompt] API response: {result[:100]}...")
                                    return (result,)
                                else:
                                    error_msg = f"API response format error: {response_data}"
                                    print(error_msg)
                                    return (error_msg,)
                            else:
                                error_msg = f"HTTP error: {response.status_code} - {response.text[:200]}"
                                print(error_msg)
                                # å¦‚æœæ˜¯æœåŠ¡å™¨é”™è¯¯ï¼ˆ5xxï¼‰ï¼Œå°è¯•é‡è¯•
                                if response.status_code >= 500 and attempt < max_retries - 1:
                                    print(f"[NakuNode-Prompt] Server error, retrying in {retry_delay} seconds...")
                                    time.sleep(retry_delay)
                                    continue
                                return (error_msg,)
                        except requests.exceptions.ConnectionError as e:
                            print(f"[NakuNode-Prompt] Connection error (attempt {attempt + 1}/{max_retries}): {str(e)}")
                            if attempt < max_retries - 1:
                                print(f"[NakuNode-Prompt] Retrying in {retry_delay} seconds...")
                                time.sleep(retry_delay)
                                retry_delay *= 2  # æŒ‡æ•°é€€é¿
                            else:
                                error_msg = f"Connection failed after {max_retries} attempts: {str(e)}"
                                print(error_msg)
                                return (error_msg,)
                        except requests.exceptions.Timeout:
                            error_msg = "API request timeout"
                            print(error_msg)
                            return (error_msg,)
                        except Exception as e:
                            error_msg = f"Unexpected error: {str(e)}"
                            print(error_msg)
                            return (error_msg,)

            except requests.exceptions.Timeout:
                error_msg = "API request timeout"
                print(error_msg)
                return (error_msg,)
            except requests.exceptions.RequestException as e:
                error_msg = f"HTTP request failed: {str(e)}"
                print(error_msg)
                import traceback
                traceback.print_exc()
                return (error_msg,)
            except Exception as e:
                error_msg = f"API call failed: {str(e)}"
                print(error_msg)
                import traceback
                traceback.print_exc()
                return (error_msg,)
        else:
            print(f"[NakuNode-Prompt] Unknown API provider: {api_provider}, not calling API")
            full_prompt = f"{system_prompt}\n\nUser request: {user_prompt}"
            return (full_prompt,)


# --- Register node to ComfyUI ---
NODE_CLASS_MAPPINGS = {
    "NakuNodePromptEVO": NakuNodePromptEVO
}
NODE_DISPLAY_NAME_MAPPINGS = {
    "NakuNodePromptEVO": "NakuNode-PromptEVO"
}
